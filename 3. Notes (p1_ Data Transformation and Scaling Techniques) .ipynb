{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Notes (p1: Data Transformation and Scaling Techniques) .ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyONegW+lbBm2JUlIaRDTCXA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"znl0ICYDU_st"},"outputs":[],"source":[""]},{"cell_type":"markdown","source":["# **Introduction:**\n","\n","\n","In my machine learning journey, more often than not, I have found that feature preprocessing is a more effective technique in improving my evaluation metric than any other step, like choosing a model algorithm, hyperparameter tuning, etc.\n","\n","  **Feature preprocessing is one of the most crucial steps in building a Machine learning model.** Too few features and your model won’t have much to learn from. Too many features and we might be feeding unnecessary information to the model. Not only this, but the values in each of the features need to be considered as well.\n","\n","  We know that there are some set rules of dealing with categorical data, as in, encoding them in different ways. However, a large chunk of the process involves dealing with continuous variables. There are various methods of dealing with continuous variables. Some of them include converting them to a normal distribution or converting them to categorical variables, etc.\n","\n","**These techniques are:**\n","\n","*  **Feature Transformation and**\n","*  **Feature Scaling.**"],"metadata":{"id":"wc4YdbCcoySA"}},{"cell_type":"markdown","source":["# **Why do we need Feature Transformation and Scaling?**\n","\n","\n","Oftentimes, we have datasets in which different columns have different units – like one column can be in kilograms, while another column can be in centimeters. Furthermore, we can have columns like income which can range from 20,000 to 100,000, and even more; while an age column which can range from 0 to 100(at the most). Thus, Income is about 1,000 times larger than age.\n","\n","But how can we be sure that the model treats both these variables equally? When we feed these features to the model as is, there is every chance that the income will influence the result more due to its larger value. But this doesn’t necessarily mean it is more important as a predictor. So, to give importance to both Age, and Income, we need feature scaling.\n","\n","In most examples of machine learning models, you would have observed either the Standard Scaler or MinMax Scaler. However, the powerful sklearn library offers many other feature transformations scaling techniques as well, which we can leverage depending on the data we are dealing with. "],"metadata":{"id":"Iad0UatGkA0T"}},{"cell_type":"markdown","source":["# **1. Data Transformation:**\n","\n","\n","\n","\n"],"metadata":{"id":"1R7ShBR4lMif"}},{"cell_type":"markdown","source":["## **1.1 MinMax Scaler:**\n","\n","The MinMax scaler is one of the simplest scalers to understand.  It just scales all the data between 0 and 1. The formula for calculating the scaled value is-\n","\n","x_scaled = (x – x_min)/(x_max – x_min)\n","\n","Thus, a point to note is that it does so for every feature separately. Though (0, 1) is the default range, we can define our range of max and min values as well."],"metadata":{"id":"-xZdCAJolO6X"}},{"cell_type":"markdown","source":["## **1.2 Standarization or Standard Scaler:**\n","\n","\n","Standarization is a scaler technique where the values are centred around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resulant distribution has a unit standard deviation.\n","\n","mean = 0\n","standard deviation = 1\n","\n","x_scaled = x – (mean/std_dev)\n","\n"],"metadata":{"id":"zHzdZOlylgTw"}},{"cell_type":"markdown","source":["## **1.3 MaxAbs Scaler:**\n","\n","In simplest terms, the MaxAbs scaler takes the absolute maximum value of each column and divides each value in the column by the maximum value.\n","\n","Thus, it first takes the absolute value of each value in the column and then takes the maximum value out of those. This operation scales the data between the range **[-1, 1]**."],"metadata":{"id":"8uA1ySqYmpGZ"}},{"cell_type":"markdown","source":["## **1.4 Robust Scaler:**\n","\n","If you have noticed in the scalers we used so far, each of them was using values like the mean, maximum and minimum values of the columns. All these values are sensitive to outliers. If there are too many outliers in the data, they will influence the mean and the max value or the min value. Thus, even if we scale this data using the above methods, we cannot guarantee a balanced data with a normal distribution.\n","\n","\n","\n","The Robust Scaler, as the name suggests is not sensitive to outliers. This scaler-\n","\n","*  removes the median from the data,\n","*  scales the data by the InterQuartile Range(IQR).\n","\n","\n","Inter-Quartile Range is nothing but the difference between the first and third quartile of the variable. The interquartile range can be defined as-\n","\n","IQR = Q3 – Q1\n","\n","Thus, the formula would be:\n","\n","x_scaled = (x – Q1)/(Q3 – Q1)\n","\n"],"metadata":{"id":"ReYI_UmfnB6H"}},{"cell_type":"markdown","source":["## **1.5 Quantile Transformer Scaler:**\n","\n","\n","One of the most interesting feature transformation techniques that I have used, the **Quantile Transformer Scaler converts the variable distribution to a normal distribution.** and scales it accordingly. Since it makes the variable normally distributed, it also deals with the outliers. Here are a few important points regarding the Quantile Transformer Scaler:\n","\n","* It computes the cumulative distribution function of the variable\n","\n","* It uses this cdf to map the values to a normal distribution\n","\n","* Maps the obtained values to the desired output distribution using the associated quantile function\n","\n"],"metadata":{"id":"8ondtbQrntUZ"}},{"cell_type":"markdown","source":["# **2. Scaling Techniques:**\n","\n"],"metadata":{"id":"JYeKUCBxobpQ"}},{"cell_type":"markdown","source":["## **2.1 Log Transform:**\n","\n","\n","The Log Transform is one of the most popular Transformation techniques out there. It is primarily used to convert a skewed distribution to a normal distribution/less-skewed distribution. In this transform, we take the log of the values in a column and use these values as the column instead.\n","\n","Why does it work? It is because the log function is equipped to deal with large numbers. Here is an example-\n","\n","log(10) = 1\n","\n","log(100) = 2, and\n","\n","log(10000) = 4.\n","\n"],"metadata":{"id":"_DJn8scNo_OR"}},{"cell_type":"markdown","source":["## **2.2 Recipocal Transformation:**\n","\n","reciVal = (1/features_value)"],"metadata":{"id":"OLySKUQ0pPuw"}},{"cell_type":"markdown","source":["## **2.3 Square Root Transformation:**"],"metadata":{"id":"ZLuO0K8LpgY_"}},{"cell_type":"markdown","source":["## **2.4. Box-Cox Transformation:**"],"metadata":{"id":"Sqq9wkMQplwf"}},{"cell_type":"markdown","source":["## **2.5 Other Custome Transformation:**"],"metadata":{"id":"guPEtngppsVn"}},{"cell_type":"code","source":[""],"metadata":{"id":"YNdAqdFbc7rN"},"execution_count":null,"outputs":[]}]}